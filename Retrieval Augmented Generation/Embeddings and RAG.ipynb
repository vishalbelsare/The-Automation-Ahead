{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings and RAG Visual Demonstrations\n",
    "This notebook visually demonstrates how embeddings work in a Retrieval-Augmented Generation (RAG) pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install Dependencies\n",
    "Before running the code, ensure you have the following libraries installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai\n",
    "!pip install numpy\n",
    "!pip install scikit-learn\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Manim\n",
    "\n",
    "Manim is a powerful Python library for creating mathematical and scientific animations, including 2D and 3D visualizations. In this demonstration, we use Manim to visualize text embeddings and their semantic relationships in a 3D space.\n",
    "\n",
    "To run the visualization code in this notebook, you need to install the Manim library. Follow the official installation instructions for your operating system:\n",
    "\n",
    "- **Installation Guide:** [Manim Installation](https://docs.manim.community/en/stable/installation.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from manim import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%manim -pql TextToVector\n",
    "\n",
    "class TextToVector(Scene):\n",
    "    def construct(self):\n",
    "        # Step 1: Display the input text\n",
    "        text = Text(\"Investing in small-cap stocks is risky but can be rewarding.\", font_size=24)\n",
    "        text.to_edge(UP)\n",
    "        self.play(Write(text))\n",
    "        self.wait(1)\n",
    "\n",
    "        # Step 2: Create a vertical list of words, positioned on the left\n",
    "        words = [\"Investing\", \"in\", \"small-cap\", \"stocks\", \"is\", \"risky\", \"but\", \"can\", \"be\", \"rewarding\"]\n",
    "        word_list = VGroup(*[Text(word, font_size=20) for word in words])\n",
    "        word_list.arrange(DOWN, aligned_edge=LEFT, buff=0.5)  # Arrange words vertically\n",
    "        word_list.to_edge(LEFT, buff=1)  # Move the list to the far left\n",
    "\n",
    "        # Animate the appearance of the word list\n",
    "        self.play(FadeIn(word_list))\n",
    "        self.wait(1)\n",
    "\n",
    "        # Step 3: Highlight the transition to the vector representation\n",
    "        vector = Matrix(\n",
    "            [[\"0.022\"], [\"-0.019\"], [\"...\"], [\"-0.054\"]],  # Example random numbers and ellipsis\n",
    "            include_background_rectangle=True\n",
    "        )\n",
    "        vector.scale(0.8)\n",
    "        vector.move_to([0, 0, 0])  # Center the vector on the screen\n",
    "\n",
    "        arrow = Arrow(word_list.get_right(), vector.get_left(), buff=0.1, color=YELLOW)\n",
    "\n",
    "        # Animate the conversion to the vector\n",
    "        self.play(Create(arrow))\n",
    "        self.play(Transform(word_list, vector))\n",
    "        self.wait(2)\n",
    "\n",
    "        # Clean up the scene\n",
    "        # self.play(FadeOut(text, word_list, arrow, vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration: Semantic Embeddings \n",
    "\n",
    "In this section, we’re going to embed a few sentences using a text embedding model and then reduce the dimensionality of these embeddings to visualize their semantic relationships.\n",
    "\n",
    "- We have two sentences related and one which is completely unrelated to the other two.\n",
    "- By converting each sentence into an embedding (a high-dimensional vector), we capture the semantic meaning of the text.\n",
    "- We then use Principal Component Analysis (PCA) to reduce the embeddings to just three dimensions, making it easier to visualize their relative positions.\n",
    "- Sentences with similar topics (small-cap investing) should end up closer together in this 3D space, while the unrelated quantum mechanics sentence should appear farther away, illustrating how embeddings group similar meanings together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To proceed, you'll need an OpenAI API key and a `.env` file to store it securely:\n",
    "\n",
    "1. **Obtain an OpenAI API Key:**\n",
    "   - Visit the [OpenAI API Keys page](https://platform.openai.com/account/api-keys) to generate your key.\n",
    "\n",
    "2. **Create a `.env` File:**\n",
    "   - Refer to [this guide](https://www.geeksforgeeks.org/how-to-create-and-use-env-files-in-python/) for instructions on creating and using `.env` files in Python.\n",
    "\n",
    "Ensure your `.env` file is in the project directory and includes your OpenAI API key in the following format:\n",
    "OPENAI_API_KEY=your_api_key_here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding and Dimensionality Reduction\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Define sentences: two related to small-cap stock investing and one unrelated\n",
    "sentences = [\n",
    "    \"Investing in smaller companies is riskier.\",\n",
    "    \"Smaller company investments come with higher risks.\",\n",
    "    \"Physics explores particle behavior.\",\n",
    "    \n",
    "]\n",
    "\n",
    "# Embed sentences\n",
    "embeddings = []\n",
    "for sentence in sentences:\n",
    "    response = client.embeddings.create(input=sentence, model=\"text-embedding-3-small\")\n",
    "    embeddings.append(response.data[0].embedding)\n",
    "\n",
    "# Convert embeddings to numpy array\n",
    "embeddings = np.array(embeddings)\n",
    "\n",
    "# Reduce dimensions to 3D using PCA \n",
    "pca = PCA(n_components=3)\n",
    "embeddings_3d = pca.fit_transform(embeddings)\n",
    "\n",
    "# Print 3D coordinates\n",
    "for sentence, coords in zip(sentences, embeddings_3d):\n",
    "    print(f\"Sentence: {sentence}\\n3D Coordinates: {coords}\\n\")\n",
    "\n",
    "# 3D coordinates for visualization\n",
    "embedding_coordinates = embeddings_3d.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Embeddings in 3D Space\n",
    "\n",
    "In this section, we visualize the embeddings of three sentences in a 3D space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%manim -pql EmbeddingVisualization\n",
    "\n",
    "class EmbeddingVisualization(ThreeDScene):\n",
    "    def construct(self):\n",
    "        axes = ThreeDAxes()\n",
    "        self.add(axes)\n",
    "\n",
    "        scale_factor = 5\n",
    "\n",
    "        colors = [BLUE, BLUE, GREEN]\n",
    "\n",
    "        for i, coords in enumerate(embedding_coordinates[:3]):\n",
    "            scaled_coords = [coord * scale_factor for coord in coords]\n",
    "            dot = Dot3D(point=scaled_coords, color=colors[i], radius=0.1)\n",
    "\n",
    "            self.add(dot)\n",
    "\n",
    "        # Set initial camera orientation\n",
    "        self.set_camera_orientation(phi=75 * DEGREES, theta=-45 * DEGREES)\n",
    "\n",
    "        # Rotate camera by 360 degrees around the scene, slower and smoother\n",
    "        self.move_camera(theta=-45 * DEGREES + TAU, run_time=30, rate_func=linear)\n",
    "        \n",
    "        self.wait(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Transcripts Visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%manim -pql TranscriptChunksToMatrix\n",
    "\n",
    "from manim import *\n",
    "\n",
    "class TranscriptChunksToMatrix(Scene):\n",
    "    def construct(self):\n",
    "        # Step 1: Create the Transcript Icon\n",
    "        transcript = SVGMobject(\"../../Retrieval Augmented Generation/Visualizations/media/images/Visualizations/svg/document-text-svgrepo-com.svg\").scale(0.7).shift(LEFT * 4)\n",
    "        transcript_label = Text(\"Transcript\", font_size=24).next_to(transcript, DOWN)\n",
    "        transcript_group = VGroup(transcript, transcript_label)\n",
    "\n",
    "        # Add transcript to the scene\n",
    "        self.play(FadeIn(transcript_group))\n",
    "        self.wait(2)\n",
    "\n",
    "        # Step 2: Define chunks and real embeddings (rounded)\n",
    "        chunks = [\"Chunk 1\", \"Chunk 2\", \"Chunk n\"]\n",
    "        vectors = [\n",
    "            [0.038, -0.010, \"...\", 0.083],\n",
    "            [0.022, 0.011, \"...\", 0.049],\n",
    "            [0.045, 0.003,\"...\", 0.007]\n",
    "        ]\n",
    "        rows = [[chunks[i], f\"[{vectors[i][0]}, {vectors[i][1]}, {vectors[i][2]}, {vectors[i][3]}]\"] for i in range(len(chunks))]\n",
    "\n",
    "        # Step 3: Create the matrix (chunks with vectors)\n",
    "        chunk_matrix = Table(\n",
    "            rows,\n",
    "            row_labels=None,\n",
    "            col_labels=[Text(\"Chunk\"), Text(\"Embeddings\")],\n",
    "            top_left_entry=None\n",
    "        ).scale(0.7).shift(LEFT * 2)\n",
    "\n",
    "        # Animate transcript transforming into chunks\n",
    "        self.play(Transform(transcript_group, chunk_matrix))\n",
    "        self.wait(2)\n",
    "\n",
    "        # Step 4: Add Database Icon to Scene\n",
    "        database_icon = SVGMobject(\"../../Retrieval Augmented Generation/Visualizations/media/images/Visualizations/svg/database-svgrepo-com.svg\").scale(0.7).shift(RIGHT * 4)\n",
    "        database_label = Text(\"Vector Database\", font_size=18).next_to(database_icon, DOWN)\n",
    "\n",
    "        # Step 5: Add arrow connecting the matrix to the database\n",
    "        arrow = Arrow(start=chunk_matrix.get_right(), end=database_icon.get_left(), buff=0.2, color=YELLOW)\n",
    "\n",
    "        # Animate the arrow and database appearance\n",
    "        self.play(GrowArrow(arrow), FadeIn(database_icon), Write(database_label))\n",
    "        self.wait(3)\n",
    "\n",
    "        # Step 6: Clean up the scene\n",
    "        self.play(FadeOut(chunk_matrix, arrow, database_icon, database_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrating Retrieval-Augmented Generation (RAG) with Sentence Embeddings\n",
    "\n",
    "In this section, we demonstrate how a Retrieval-Augmented Generation (RAG) pipeline works using a simple query and a set of text chunks. This approach combines embeddings, nearest neighbor search, and visualization to retrieve semantically similar information.\n",
    "\n",
    "#### Workflow Overview:\n",
    "1. **Query and Chunks:**  \n",
    "   - A query (e.g., *\"What are the risks and benefits of investing in small-cap stocks?\"*) is provided.  \n",
    "   - Several chunks of information are defined, each discussing different investment topics.\n",
    "\n",
    "2. **Embedding Sentences:**  \n",
    "   - Each sentence (query and chunks) is converted into a high-dimensional vector representation using a text embedding model.  \n",
    "   - These embeddings encode the semantic meaning of the sentences.\n",
    "\n",
    "3. **Reducing Dimensions:**  \n",
    "   - To make the high-dimensional embeddings easier to visualize, we reduce their dimensions to 3D using Principal Component Analysis (PCA).  \n",
    "   - This allows us to plot the embeddings in a 3D space.\n",
    "\n",
    "4. **Finding Similar Chunks (k-Nearest Neighbors):**  \n",
    "   - Using the query’s embedding, we search for the top `k=3` nearest neighbors (most semantically similar chunks) among the other embeddings.  \n",
    "   - Nearest neighbors are identified using a Euclidean distance metric.\n",
    "\n",
    "5. **Visualization:**  \n",
    "   - The query is plotted as a red dot.  \n",
    "   - The two closest chunks are highlighted in yellow.  \n",
    "   - All other chunks are plotted in blue.  \n",
    "   - Labels are added to make it easy to understand which point corresponds to which sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Define query and chunks\n",
    "query = \"what are some of the market risks that could potentially impact near-term EBITDA?\"\n",
    "\n",
    "relevant = [\"Most commodities are going to move down in price while NAND and DRAM increased during the course of the September quarter, and we expect them to increase during the December quarter.\",\n",
    "    \"If you look at how we did for the quarter in China, we were relatively flat year over year, and a key component of that improvement relative to the year-over-year performance that we had been achieving is that there was a sequential improvement in foreign exchange\"]\n",
    "\n",
    "irrelevant = [\"And we love celebrating the craft of great storytellers who know how to put on a show.\",\n",
    "    \"I had an incredible time during launch day in September alongside our team at Apple Fifth Avenue where energy and enthusiasm filled the air.\"\n",
    "    \"Today, users choose Apple Pay for purchases across tens of millions of retailers worldwide.\",\n",
    "    \"In honor of World Teachers' Day, Apple was proud to share new resources for teachers to engage their students in ways that aim to make learning easy and fun.\",\n",
    "    \"With AirPods 4, we’ve broken new ground in comfort and design with our best-ever open-ear headphones available for the first time with active noise cancellation\",\n",
    "    \"The iPhone active installed base grew to a new all-time high in total and in every geographic segment\",\n",
    "    \"The latest reports from 451 Research indicated customer satisfaction of 96% for Watch in the U.S.\"]\n",
    "\n",
    "chunks = relevant + irrelevant\n",
    "\n",
    "# Create labels: \"Q\" for query, and \"A\", \"B\", ... for the chunks\n",
    "labels = [\"Q\"] + [chr(ord('A') + i) for i in range(len(chunks))]\n",
    "\n",
    "# Embed sentences (Assume `client` is already initialized)\n",
    "all_sentences = [query] + chunks\n",
    "embeddings = []\n",
    "for sentence in all_sentences:\n",
    "    response = client.embeddings.create(\n",
    "        input=sentence,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    embeddings.append(response.data[0].embedding)\n",
    "\n",
    "# Convert embeddings to numpy array\n",
    "embeddings = np.array(embeddings)\n",
    "\n",
    "# Reduce dimensions to 3D for visualization\n",
    "pca = PCA(n_components=3)\n",
    "embeddings_3d = pca.fit_transform(embeddings)\n",
    "\n",
    "# Print 3D coordinates for debugging\n",
    "for label, sentence, coords in zip(labels, all_sentences, embeddings_3d):\n",
    "    print(f\"Label: {label}\\nSentence: {sentence}\\n3D Coordinates: {coords}\\n\")\n",
    "\n",
    "# Find k=2 nearest neighbors for the query\n",
    "query_embedding = embeddings[0].reshape(1, -1)  # Query is at index 0\n",
    "chunk_embeddings = embeddings[1:]  # Chunks start from index 1\n",
    "\n",
    "knn = NearestNeighbors(n_neighbors=2, metric='euclidean')\n",
    "knn.fit(chunk_embeddings)\n",
    "distances, indices = knn.kneighbors(query_embedding)\n",
    "\n",
    "# Map nearest neighbor indices back to labels\n",
    "closest_chunks = [labels[i+1] for i in indices[0]]  # +1 because query is index 0\n",
    "print(\"The 2 nearest neighbors to the query are:\", closest_chunks)\n",
    "\n",
    "# Prepare 3D embedding coordinates for visualization\n",
    "embedding_coordinates = embeddings_3d.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Embeddings Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%manim -pql RAGEmbeddingVisualization\n",
    "\n",
    "class RAGEmbeddingVisualization(ThreeDScene):\n",
    "    def construct(self):\n",
    "        axes = ThreeDAxes()\n",
    "        self.add(axes)\n",
    "\n",
    "        scale_factor = 5\n",
    "\n",
    "        # Plot the query in RED\n",
    "        query_coords = embedding_coordinates[0]\n",
    "        query_point = [coord * scale_factor for coord in query_coords]\n",
    "        query_dot = Dot3D(point=query_point, color=RED, radius=0.06)\n",
    "        query_label = Text(labels[0], font_size=24).move_to(query_dot.get_center())\n",
    "        self.add_fixed_orientation_mobjects(query_label)\n",
    "        self.add(query_dot, query_label)\n",
    "\n",
    "        # Plot the chunks\n",
    "        for i, coords in enumerate(embedding_coordinates[1:], start=1):\n",
    "            scaled_coords = [coord * scale_factor for coord in coords]\n",
    "            dot_color = YELLOW if labels[i] in closest_chunks else BLUE\n",
    "            dot = Dot3D(point=scaled_coords, color=dot_color, radius=0.05)\n",
    "            chunk_label = Text(labels[i], font_size=24).move_to(dot.get_center())\n",
    "            self.add_fixed_orientation_mobjects(chunk_label)\n",
    "            self.add(dot, chunk_label)\n",
    "\n",
    "        # Initial camera orientation\n",
    "        self.set_camera_orientation(phi=75 * DEGREES, theta=-45 * DEGREES)\n",
    "\n",
    "        # Rotate camera 360 degrees slowly and linearly\n",
    "        self.move_camera(theta=-45 * DEGREES + TAU, run_time=30, rate_func=linear)\n",
    "        \n",
    "        self.wait(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-4o Response Without Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. {context}\"\n",
    "\n",
    "system_prompt = ChatPromptTemplate.from_template(template)\n",
    "    \n",
    "\n",
    "final_prompt = system_prompt.format(context=\"/n\".join(relevant))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. Most commodities are going to move down in price while NAND and DRAM increased during the course of the September quarter, and we expect them to increase during the December quarter./nIf you look at how we did for the quarter in China, we were relatively flat year over year, and a key component of that improvement relative to the year-over-year performance that we had been achieving is that there was a sequential improvement in foreign exchange'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = openai.OpenAI()\n",
    "# Define the query\n",
    "question = \"What are some of the market risks that could potentially impact Apple's near-term EBITDA? Give your answer in two sentence.\"\n",
    "\n",
    "# Function to get response from GPT-4\n",
    "def ask_gpt4(system_prompt,question):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": question},\n",
    "            ],\n",
    "            max_tokens=500,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "# Get and print the response\n",
    "response = ask_gpt4(final_prompt,question)\n",
    "print(\"GPT-4 Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context from the 4th quarter 2024 earnings call to answer the question. \"\n",
    "    \"If you don't know the answer, say that you don't know.\\n\\n\"\n",
    "    f\"{' '.join(context for context in relevant)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4 Response:\n",
      "Some market risks that could potentially impact Apple's near-term EBITDA include fluctuations in foreign exchange rates, which can affect revenue and profitability, particularly in international markets like China. Additionally, rising costs for NAND and DRAM components could increase expenses and impact margins.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = openai.OpenAI()\n",
    "# Define the query\n",
    "query = \"What are some of the market risks that could potentially impact Apple's near-term EBITDA? Give your answer in two sentence.\"\n",
    "\n",
    "# Function to get response from GPT-4\n",
    "def ask_gpt4(question):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": question},\n",
    "            ],\n",
    "            max_tokens=500,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "# Get and print the response\n",
    "response = ask_gpt4(query)\n",
    "print(\"GPT-4 Response:\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
