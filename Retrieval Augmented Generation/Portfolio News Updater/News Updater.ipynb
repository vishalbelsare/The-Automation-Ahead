{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e56619e4",
      "metadata": {},
      "source": [
        "# Portfolio Material News Updater  \n",
        "\n",
        "Welcome to the **Material News Updater** tutorial. By the end of this notebook you will be able to:\n",
        "\n",
        "1. **Collect** fresh articles from a curated list of financial-news RSS feeds.\n",
        "2. **Embed** those stories in a Chroma vector database with OpenAI embeddings.\n",
        "3. **Query** the database with GPT-4.1-mini to surface *material* news for a portfolio of stocks.\n",
        "4. **Summarize** the individual stock news breifs into a concise portfolio brief.\n",
        "5. **Email** yourself a concise morning briefing—fully automated.\n",
        "\n",
        "You’ll practise:\n",
        "\n",
        "* Working with RSS feeds.\n",
        "* Turning raw text into embeddings using LangChain + OpenAI.\n",
        "* Building a Retrieval-Augmented Generation (RAG) chain.\n",
        "* Packaging results for convenient distribution.\n",
        "\n",
        "**Prerequisites**  \n",
        "• Python ≥ 3.11 (Conda or venv).  \n",
        "• OpenAI API key and Yahoo Mail credentials stored in a `.env` file (use the .env.example template and remove the .example suffix).\n",
        "\n",
        "# OpenAI Account Setup\n",
        "* Create an OpenAI Platform Account – Go to [platform.openai.com](https://platform.openai.com/docs/overview) and follow the prompts to sign up.\n",
        "* Generate Your API Key – After signing in, navigate to the API keys page to create a new secret key.\n",
        "\n",
        "# Yahoo Account Setup\n",
        "This example uses **Yahoo Mail** to send the morning brief because it supports SMTP with `STARTTLS`, making it easy to integrate using the stmplib python library for sending emails.\n",
        "\n",
        "⚠️ **WARNING:** For security and privacy reasons, we strongly recommend using a **dedicated “junk” Yahoo account** for this project. Do **not** use your personal or sensitive email address as your sending email. The TO_EMAIL, which is the email for the brief to be sent to can be any email as this does not require sensitive information. \n",
        "\n",
        "To set it up:\n",
        "- [Create a new Yahoo email account](https://login.yahoo.com/account/create)\n",
        "- [Generate an app password](https://help.yahoo.com/kb/SLN15241.html?guccounter=1) to use instead of your actual login\n",
        "\n",
        "You can also run this notebook via a Google Colab here: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1nEkt2ivAlU2XVU7AkyQG4Q-rMat7UcX8?usp=sharing)\n",
        "\n",
        "---\n",
        "\n",
        "Let’s start by loading the RSS catalogue."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8723b736",
      "metadata": {},
      "source": [
        "## 1. Load RSS sources\n",
        "\n",
        "`news_rss.json` contains a simple list of feed URLs plus some lightweight metadata.  \n",
        "The next cell just deserialises that file into Python so we can iterate over it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "e43e0230",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open('news_rss.json', 'r') as file:\n",
        "    rss_json = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5694b04f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unhash the below line to install the feedparser package\n",
        "# !pip install feedparser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "a83c9bbc",
      "metadata": {},
      "outputs": [],
      "source": [
        "import feedparser\n",
        "import time\n",
        "\n",
        "feeds = []\n",
        "\n",
        "for rss_dict in rss_json:\n",
        "    rss_url = rss_dict['rss']\n",
        "    source = rss_dict['source']\n",
        "    news_type = rss_dict['type']\n",
        "    feed = feedparser.parse(rss_url)\n",
        "    feed_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(time.time()))\n",
        "    feed_dict = {\n",
        "        'source':source,\n",
        "        'type':news_type,\n",
        "        'feed':feed,\n",
        "        'time_pulled':feed_time\n",
        "    }\n",
        "    feeds.append(feed_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f86a463d",
      "metadata": {},
      "source": [
        "## 2. Embed stories and build the vector store\n",
        "  \n",
        "Here we walk through each RSS entry, keep the entire article intact (because they’re short), and prepare a list of `documents` that we’ll feed into Chroma. Each document carries metadata—`title`, `source`, the time we pulled it, and a high-level `news_type` tag—for easier filtering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "8ec97ca7",
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Create full documents from entries without splitting\n",
        "documents = []\n",
        "for rss_feed in feeds:\n",
        "    try:\n",
        "        for entry in rss_feed['feed']['entries']: \n",
        "            title = entry['title']\n",
        "            entry_content = str(entry)\n",
        "            documents.append({\"content\": entry_content, \"metadata\": {\"title\":title,\"source\": rss_feed[\"source\"], \"time_pulled\": rss_feed[\"time_pulled\"], \"news_type\":rss_feed['type']}})\n",
        "    except Exception as e:\n",
        "        print(f'An error - {e} occured for {entry}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64efc379",
      "metadata": {},
      "source": [
        "### Batch The Embeddings\n",
        "\n",
        "OpenAI’s embedding endpoint caps *each request* at **300 k tokens**. The helper below measures every document, adds them to a running total, and starts a new batch whenever the next doc would tip us over the limit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "929c60f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unhash and run the below if you need to install the tiktoken package\n",
        "# !pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "3895a4bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def batch_docs(documents, model='text-embedding-3-small', token_limit=300000):\n",
        "    import tiktoken\n",
        "    encoding = tiktoken.encoding_for_model(model)\n",
        "    \n",
        "    batches = []\n",
        "    current_tokens = 0\n",
        "    current_docs = []\n",
        "\n",
        "    for document in documents:\n",
        "        tokens = len(encoding.encode(document[\"content\"]))\n",
        "        \n",
        "        if tokens > token_limit:\n",
        "            raise ValueError(f\"Single document exceeds token limit: {tokens} tokens\")\n",
        "\n",
        "        if current_tokens + tokens > token_limit:\n",
        "            batches.append(current_docs)\n",
        "            current_docs = [document]\n",
        "            current_tokens = tokens\n",
        "        else:\n",
        "            current_docs.append(document)\n",
        "            current_tokens += tokens\n",
        "\n",
        "    if current_docs:\n",
        "        batches.append(current_docs)\n",
        "\n",
        "    return batches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "9796a370",
      "metadata": {},
      "outputs": [],
      "source": [
        "batches = batch_docs(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "885b1d07",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unhash the below to install chromadb\n",
        "# !pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "65635ab2",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# Initialize the embedding model and vector store\n",
        "embedding_model = OpenAIEmbeddings(model='text-embedding-3-small')\n",
        "vector_store = Chroma(collection_name='rss_news_feeds', embedding_function=embedding_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "bb81c541",
      "metadata": {},
      "outputs": [],
      "source": [
        "for batch in batches:\n",
        "    texts = [doc[\"content\"] for doc in batch]\n",
        "    metadatas = [doc[\"metadata\"] for doc in batch]\n",
        "    vector_store.add_texts(texts, metadatas=metadatas)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "371c3dae",
      "metadata": {},
      "source": [
        "## 3. Craft the system prompt\n",
        "\n",
        "This prompt underpins our retrieval chain. We must include the \"{context}\" variable in the prompt as this will bring in the relevant news articles based on our query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "2a9a6d25",
      "metadata": {},
      "outputs": [],
      "source": [
        "system_prompt = (\n",
        "    \"You are an assistant designed to search through news feeds and find the most relevant news based on a prompt. \"\n",
        "    \"Use the following pieces of retrieved context to answer \"\n",
        "    \"the question. If you don't have enough information to infer an answer, say that you dont have enough information to answer the question.\"\n",
        "    \"Each document contains the meta data which has the source, news type and pulled date. If you have two stories from different sources that\"\n",
        "    \" appear to be about the same story prioritize the one with the ealier published time.\"\n",
        "    \"Keep the answer concise.\"\n",
        "    \"\\n\\n\"\n",
        "    \"{context}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7adf4713",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unhash the below line if you need to install yfinance\n",
        "#!pip install yfinance # We will use yfinance to give some more context about portfolio companies"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "099749a0",
      "metadata": {},
      "source": [
        "## 4. Create the RAG Chain\n",
        "\n",
        "Here we put all the components together for our RAG workflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bec75a2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unhash the below to install langchain_community and langchain_openai\n",
        "#!pip install langchain_community\n",
        "#!pip install langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "d2e3bf2f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from datetime import datetime\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": 20})\n",
        "qa_chain = create_stuff_documents_chain(llm, prompt)\n",
        "rag_chain = create_retrieval_chain(retriever, qa_chain)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rag_over_portfolio",
      "metadata": {},
      "source": [
        "## 5. Create material news summaries for each company\n",
        "\n",
        "For each ticker we:\n",
        "1. Pull a short business description from `yfinance` for extra context.  \n",
        "2. Ask the RAG chain which news **published today** might have a material effect on the stock prices.  \n",
        "3. Put each answer in a document for us to aggregate into our portfolio brief later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "98482519",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.documents import Document\n",
        "from typing import List\n",
        "from openai import OpenAI\n",
        "import yfinance as yf\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "# 1. Function to summarize news for a given ticker\n",
        "def summarize_ticker(ticker: str) -> str:\n",
        "    today = datetime.today().strftime(\"%Y-%m-%d\")\n",
        "    info  = yf.Ticker(ticker).info\n",
        "    company     = info.get(\"longName\", ticker)\n",
        "    description = info.get(\"longBusinessSummary\", \"\")\n",
        "    query = (\n",
        "        f\"Today's date is {today}. You MUST only reference news published either today or yesterday. News published any other day is unacceptable!\\n\\n\"\n",
        "        f\"Here is a description of {company}: {description}\\n\\n\"\n",
        "        f\"Which news stories published either today or yesterday are most likely to have a material impact on {company}'s stock? If there isnt anything material, say so.\\n\\n\"\n",
        "        \"Include citations. Remember news published any other day is unacceptable!\"\n",
        "    )\n",
        "    result = rag_chain.invoke({\"input\": query})\n",
        "    return f\"{ticker} – {company}:\\n{result}\\n\"\n",
        "\n",
        "# 2. Generate per-ticker summaries\n",
        "tickers = [\"AAPL\",\"MSFT\",\"NVDA\",\"JNJ\",\"UNH\",\"JPM\",\"V\",\"PG\",\"KO\",\"XOM\"]\n",
        "summaries = [summarize_ticker(t) for t in tickers]\n",
        "\n",
        "# 3. Wrap into Documents for summarization chain\n",
        "docs = [\n",
        "    Document(page_content=s, metadata={\"ticker\": s.split(\"–\")[0]})\n",
        "    for s in summaries\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rollup_brief",
      "metadata": {},
      "source": [
        "## 6. Roll-up briefing\n",
        "\n",
        "A second LLM pass condenses the individual blurbs into a single portfolio brief, formatted in Markdown which we can easily convert to html for the email."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aff2cebf",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def create_portfolio_breif(summary_docs:List[Document],model='gpt-4o') -> str:\n",
        "    input = f\"\"\"You are a financial analyst and expert Markdown formatter.\n",
        "\n",
        "        Your task is to synthesize the following stock-specific news briefs into a structured, markdown-formatted report. Focus only on **material information** relevant to the portfolio.\n",
        "\n",
        "        **Instructions:**\n",
        "        - Structure the output using **headings** for each portfolio stock (e.g., ## AAPL).\n",
        "        - Under each stock, use **bullet points** for each insight.\n",
        "        - Use your expert financial skill to determine what actually has a high probability to be material and only highlight those stories. Be selective!\n",
        "        - Povide a clear explaination below the news on why the news could materially affect the stock - what are the potential outcomes? \n",
        "        - Add **in-context citations** (e.g., (Source: Bloomberg, June 14)) with links to back up each point.\n",
        "        - Do **not** wrap the output in code blocks.\n",
        "        - Do **not** include commentary or filler—just structured insights.\n",
        "\n",
        "        Here are the news briefs: {str(docs)}\"\"\"\n",
        "    \n",
        "    completion = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert financial analyst.\"},\n",
        "            {\"role\": \"user\", \"content\": input}\n",
        "        ],\n",
        "    )\n",
        "    return completion.choices[0].message.content\n",
        "# 3. Create and run a map-reduce summarization chain\n",
        "portfolio_brief = create_portfolio_breif(docs)\n",
        "\n",
        "print(portfolio_brief)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae218502",
      "metadata": {},
      "source": [
        "## 7. Deliver the brief\n",
        "\n",
        "We convert the Markdown to HTML and send it via Yahoo SMTP.  \n",
        "Make sure you’ve set `YAHOO_EMAIL`, `YAHOO_APP_PASSWORD`, and `TO_EMAIL` in your environment *before* running the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5291d3c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unhash the bwlow line if you need to install the markdown package\n",
        "#!pip install markdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "9a11ab1e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Email sent successfully to brian.pisaneschi@cfainstitute.org\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import smtplib\n",
        "import markdown\n",
        "from email.message import EmailMessage\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables (e.g. from a .env file)\n",
        "load_dotenv()\n",
        "\n",
        "YAHOO_EMAIL        = os.environ['YAHOO_EMAIL']\n",
        "YAHOO_APP_PASSWORD = os.environ['YAHOO_APP_PASSWORD']\n",
        "TO_EMAIL           = os.environ['TO_EMAIL']\n",
        "\n",
        "def send_yahoo_email(subject: str, body_markdown: str):\n",
        "    \"\"\"Send an HTML email via Yahoo SMTP (STARTTLS on port 587).\"\"\"\n",
        "    # Build the email\n",
        "    msg = EmailMessage()\n",
        "    msg['Subject'] = subject\n",
        "    msg['From']    = YAHOO_EMAIL\n",
        "    msg['To']      = TO_EMAIL\n",
        "\n",
        "    # Convert Markdown to HTML\n",
        "    html_body = markdown.markdown(body_markdown)\n",
        "    msg.set_content(\"This email contains HTML only.\", subtype=\"plain\")\n",
        "    msg.add_alternative(html_body, subtype=\"html\")\n",
        "\n",
        "    try:\n",
        "        # Connect, secure with STARTTLS, login, send\n",
        "        with smtplib.SMTP('smtp.mail.yahoo.com', 587, timeout=30) as server:\n",
        "            server.starttls()\n",
        "            server.login(YAHOO_EMAIL, YAHOO_APP_PASSWORD)\n",
        "            server.send_message(msg)\n",
        "        print(f\"\\n✅ Email sent successfully to {TO_EMAIL}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ Failed to send: {e}\")\n",
        "\n",
        "# Example invocation using your portfolio brief\n",
        "send_yahoo_email(\"Morning Brief\", portfolio_brief)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a01bbde",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Want to Fully Automate This Workflow?\n",
        "\n",
        "If you're interested in automating this daily news briefing—complete with scheduled runs and email delivery via GitHub Actions follow the instructions in the README file [here](https://github.com/CFA-Institute-RPC/The-Automation-Ahead/tree/main/Retrieval%20Augmented%20Generation/Portfolio%20News%20Updater/automation_scripts#readme). It walks you through setting up environment variables, customizing your portfolio, and deploying the automated workflow."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
